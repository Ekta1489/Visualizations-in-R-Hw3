---
title: "p8105_hw3_Ec3342"
author: "Ekta Chaudhary"
date: "12/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(forcats)
```

# Question 1

##Loading the instacart dataset. Counting the number of aisles using the count function. The total number of isles are given by aisle_order_count. Finding out which aisles are the most items ordered from. 

```{r instacart}

library(p8105.datasets)
data("instacart")
insta_aisles = instacart %>% 
  count(aisle, name = "aisle_order_count") %>% #counting the number of aisles 
  arrange(desc(aisle_order_count)) #arranging the aisle_order_count by descending

# The maximum number of orders are placed from the fresh vegetables and fresh fruits aisle. 
```

#Making a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered by using the filter function to limit this to aisles with more than 10000 items. aisle name is shown on y axis so that it is more legible.

```{r ggplot}

insta_aisles %>% 
  filter(aisle_order_count > 10000) %>% 
  ggplot(aes(x = aisle_order_count, y = aisle)) + geom_point()

```
#Creating a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.Including the number of times each item is ordered.

```{r popular items}

instacart_new = instacart %>% 
  select(aisle_id, aisle, product_name) %>% #selected the variables aisle_id, aisle, product_name
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  #Filtered to include the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
  count(aisle, product_name) %>% 
  group_by(aisle) %>% 
  top_n(3) 

```
#Creating a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; formating this table for human readers (i.e. produce a 2 x 7 table).

```{r icecream_apples}

icecream_apples = instacart %>% 
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(
    order_dow, order_hour_of_day, product_name) %>%
  arrange(
    desc(order_dow)) %>% 
  group_by(
    order_dow) %>% 
  summarise(
    average_order_hour_of_day = mean(order_hour_of_day)) %>% 
  mutate(
    order_dow = recode(
      order_dow, `1` = 'Mon', `2` = 'Tues', `3` = 'Wed', `4` = 'Thur', `5` = 'Fri', `6` = 'Sat', `0` = 'Sun')) %>% 
  pivot_wider(
    names_from = order_dow, values_from = average_order_hour_of_day)
kable(icecream_apples)

```


#Description of the instacart dataset!

* The instacart dataset consists of `r nrow(instacart)` rows and `r ncol(instacart)` columns.
* Some of the key variables are order_id, product_name, aisle, aisle_id, order_number
* The total number of aisles are 134.
* The maximum number of orders are placed from the fresh vegetables and fresh fruits aisle. 
* The 3 most popular items ordered from these aisles are: Baking ingredients: Light brown sugar (499), pure baking soda (387), and cane sugar (336); dog food care: Snack Sticks Chicken & Rice Recipe Dog Treats (30), Organix Chicken & Brown Rice Recipe (28), Small Dog Biscuits (26); packaged vegetables fruits: Organic Baby Spinach (9784), Organic Raspberries (5546), Organic Blueberries (4966)

# Question 2
##Loading the BRFSS data and cleaning the data. Formatted the data to use appropriate variable names. Filtered the topic to "Overall Health", included only responses from “Excellent” to “Poor”, Organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```{r health}

data("brfss_smart2010") 
health = brfss_smart2010 %>% 
  janitor::clean_names() %>% #formatted the data to use appropriate variable names
  filter(
    topic == "Overall Health"
    ) %>% #focused on the “Overall Health” topic
  filter(
    response %in% c("Excellent","Very good","Good","Fair","Poor")
    ) %>% #included only responses from “Excellent”to “Poor”
  mutate(
    response = as.factor(response),
    response = fct_relevel(
      response, "Poor","Fair","Good","Very good","Excellent")
    ) %>% #organized responses as factors. 
  arrange(response) 

```

##In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r states}

states_02 = 
  brfss_smart2010 %>%  
  janitor::clean_names() %>% #formatted the data to use appropriate variable names
  select(
    locationabbr, locationdesc, year
    ) %>% 
  filter(
    year == "2002"
    ) %>% #filtered to include only the year 2002
  group_by(
    locationabbr
    ) %>% 
  summarise(
    n_location = n_distinct(locationdesc) 
    ) %>%
  filter(n_location >= 7)

states_10 = 
  brfss_smart2010 %>%  
  janitor::clean_names() %>% #formatted the data to use appropriate variable names
  select(
    locationabbr, locationdesc, year) %>% 
  filter(
    year == "2010" #filtered to include only the year 2010
    ) %>%
  group_by(
    locationabbr
    ) %>% 
  summarise(
    n_location = n_distinct(locationdesc)
    ) %>%
  filter(n_location >= 7)

```
* In 2002, the states observed at 7 or more locations are : CT, FL, MA, NC, NJ, PA
* In 2010, the states observed at 7 or more locations are : CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA

#Constructed a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.

```{r excellent}

excellent = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  select(
    year, locationabbr, response, data_value
    ) %>% 
  filter(
    response == "Excellent"
    ) %>%
 group_by(
   year, locationabbr
   ) %>% 
  summarise(
    mean_val = mean(data_value)
    )
```

#Making a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years

```{r spaghetti plot}

ggplot(excellent, aes(x = year, y = mean_val, color = factor(locationabbr))) +   geom_line() +   theme_bw()

```
```{r}
plot = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(
    year %in% c("2006","2010")
  ) %>%
  select(
    year, locationabbr, response, data_value, locationdesc
    ) %>%
  drop_na() %>%
  filter(response %in% c("Excellent", "Very good", "Good", "Fair","Poor")
  ) %>%
  filter(locationabbr == "NY") %>%
  group_by(year, locationdesc, locationabbr) %>%
  ggplot(aes(x = response, y = data_value, fill = locationdesc)) +
  geom_bar(start = "identity", position = "dodge") +
  facet_grid(year ~.) +
  labs(x = "Response", y = "Data Value", title = "Distribution of responses among locations in NY")

```


# Question 3
#Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r}
accel_data = read_csv(file = "./data/accel_data.csv") %>% #reading the csv file
janitor::clean_names() %>% #cleaning variable names 
mutate(
  wkday_wkend = factor((day %in% c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday')),levels = c(FALSE, TRUE), labels = c('Weekend', 'Weekday')) #Added a weekday vs weekend variable called wkday_wkend
) %>%
select(week, day_id, day, wkday_wkend, everything()) #selected the variables so that the table is more readable

```

#Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r activity}
accel_data %>%
mutate(
  total_act = rowSums(select(.,activity_1:activity_1440)) #created a total_act variable that aggregates across minutes
  ) %>%
  select(week, day_id, day, wkday_wkend, total_act) %>% 
  mutate(
    day_int = case_when(
      day == "Monday" ~ 1,
      day == "Tuesday" ~ 2,
      day == "Wednesday" ~ 3,
      day == "Thursday" ~ 4,
      day == "Friday" ~ 5,
      day == "Saturday" ~ 6,
      day == "Sunday" ~ 7
    )) %>%
  arrange(week, day_int) %>% 
  select(week, day, total_act) %>%
  pivot_wider(names_from = week, values_from = total_act) %>%
  kable()
```

#There is no apparent trend seen, looking at the table. 

#Making a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r, warning=FALSE, message=FALSE}

accel_data %>%
  mutate(total_act = rowSums(select(.,activity_1:activity_1440))
         ) %>%
  select(
    week, day_id, day, wkday_wkend, total_act
    ) %>%
  ggplot(
    aes(x = week, y = total_act, color = day)
    ) +
  geom_point(alpha = 0.5) +
  geom_smooth(size = 1, se = FALSE) +
  theme_classic() +
  theme(legend.position = "bottom") +
  labs(x = "Week", y = "Activity count per day") +
  scale_y_continuous(labels = scales::comma)
  
```


 
 

